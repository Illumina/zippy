import resource
import glob
import string
import os
import sys
import fnmatch
from collections import defaultdict, namedtuple
from functools import wraps
from abc import ABCMeta, abstractmethod
import itertools
import numpy

from .utils import sub_check_wilds
from pyflow import WorkflowRunner

from star import SingleStarFlow
from bwa import BWAWorkflow
from samplesheet import SampleSheet, check_valid_samplename
from bcl2fastq import Bcl2Fastq

SampleTuple = namedtuple('SampleTuple', ['id', 'name'])
zippy_dir = os.path.dirname(__file__)


def passthrough(*all_pass):
    """
    Function decorator used to forward outputs.
    """
    def passthrough_decorator(get_output):
        @wraps(get_output)
        def func_wrapper(self, sample):
            output = get_output(self, sample)
            for to_pass in all_pass:
                pass_in = self.collect_input(sample, to_pass)
                if pass_in is not None and len(pass_in) > 0:
                    output[to_pass] = pass_in
            return output
        return func_wrapper
    return passthrough_decorator

def organize_fastqs(fastq_files, is_paired_end=True):
    '''
    Helper function used to separate R1/R2 
    '''
    r1_files = [x for x in fastq_files if '_R1_' in x]
    r2_files = [x for x in fastq_files if '_R2_' in x]
    if is_paired_end and len(r1_files)>0 and len(r2_files)>0:
        assert not set(r1_files).intersection(set(r2_files))
        return (r1_files, r2_files)
    else:
        return fastq_files

class ModularRunner():
    '''
    Naming convention: a collect method is called at the front end of a stage to take in data.
    A get method is called at the back end of a stage to give data to the next stage.
    typically, collect methods should never need to be overridden.  Get methods must be changed when
    the stage does something unusual (such as merging or creating samples).
    '''
    __metaclass__ = ABCMeta

    def __init__(self, identifier, params, previous_stages):
        self.identifier = identifier
        self.params = params
        self.previous_stages = previous_stages
        self.set_up_optional_params()
        try:
            self.sample_sheet = SampleSheet(self.params.sample_sheet, fix_dup_sample_names=self.params.optional.zippy_fix_duplicate_sample_names)
        except AttributeError:
            self.sample_sheet = None

    def set_up_optional_params(self):
        overrides = self.define_optionals()
        self.params.self._update_from_dict(overrides)
        overrides = self.define_zippy_optionals()
        self.params._update_from_dict(overrides)

    def define_optionals(self):
        '''
        Overrideable method that can be used to return a map of default values.  If these parameters
        are not set in the json file, they will be added here
        '''
        return {}

    def define_zippy_optionals(self):
        '''
        Zippy's built-in parameters have their defaults given here.
        '''
        return {'zippy_fix_duplicate_sample_names': False}

    @abstractmethod
    def get_output(self, sample):
        '''
        This function is called to return the output generated by this stage.  
        'sample' is the sample named_tuple for that sample
        The form of the output is a dictionary from zippy type to file path.  See the
        wiki for more details.
        '''
        pass

    def get_dependencies(self, sample):
        '''
        This function is called for a stage to return its own outgoing tasks related to a sample.
        I.e., this is the list of workflow tasks that must be run before the stage is complete for
        that sample.  By default, we assume that self.task is a 
        dict (or, often, a defaultdict of lists).  This can be overloaded for more complicated tasks.
        '''
        return self.task[sample]

    @abstractmethod
    def workflow(self, workflowRunner):
        '''
        Add all of this stage's tasks to the workflow graph using the input workflowRunner.
        '''
        pass

    def get_memory_count(self, count):
        '''
        Checks the default core count through the filter of a global max memory, and a in-stage set parameter.  Memory is defined in MB
        '''
        if hasattr(self.params.self, 'memory'):
            count = self.params.self.optional.memory
        if hasattr(self.params, 'max_memory'):
            count =  min(count, self.params.optional.max_memory)
        return count

    def get_core_count(self, count):
        '''
        Checks the default core count through the filter of a global max cores count, and a in-stage set parameter
        '''
        if hasattr(self.params.self, 'cores'):
            count = self.params.self.optional.cores
        if hasattr(self.params, 'max_cores'):
            count =  min(count, self.params.optional.max_cores)
        return count

    def get_samples(self):
        '''
        Returns a list of all sample tuples that this stage returns AS OUTPUT.  By default, stages do not change the 
        sample list, and so we just chain our samples along our previous stages by calling collect samples.
        '''
        return self.collect_samples()

    def collect_samples(self):
        '''
        Returns a list of all sample tuples that this stage must process by taking the union of all previous stages.
        '''
        samples = set()
        for previous_stage in self.previous_stages:
            samples |= set(previous_stage.get_samples())
        return samples

    def setup_workflow(self, workflowRunner):
        '''
        If the skip param for this stage is set to true, we don't add its workflow.
        '''
        try:
            skip = self.params.self.optional.skip
        except AttributeError:
            self.workflow(workflowRunner)
        else:
            if skip:
                self.get_dependencies=lambda x: []
            else:
                self.workflow(workflowRunner)

    def collect_input(self, sample, file_type, as_list=False):
        '''
        For input, we transparently return either a single instance or a list of 2 or more instances, depending on what we have.
        '''
        results = []
        for previous_stage in self.previous_stages:
            try:
                stage_results = previous_stage.get_output(sample)[file_type]
                if isinstance(stage_results, list):
                    results.extend(stage_results)
                elif isinstance(stage_results, str):
                    results.append(stage_results)
                elif isinstance(stage_results, unicode):
                    results.append(str(stage_results))
                else:
                    raise TypeError('Input for {} received neither a string or list: {}'.format(self.identifier, stage_results))
            except (KeyError,TypeError):
                continue
        if len(results) > 1:
            results = list(set(results)) #we want to remove duplicates that got to this point through multiple paths (e.g., passthroughs)
            if len(results) > 1 or as_list:
                return results
            else:
                return results[0]
        if len(results) == 1:
            if as_list:
                return results
            else:
                return results[0]
        else:
            return None

    def collect_dependencies(self, sample):
        dependencies = []
        for previous_stage in self.previous_stages:
            new_dependencies = previous_stage.get_dependencies(sample)
            if isinstance(new_dependencies, list):
                dependencies.extend(new_dependencies)
            elif isinstance(new_dependencies, str):
                dependencies.append(new_dependencies)
            elif isinstance(new_dependencies, unicode): #python 2 = derp
                dependencies.append(str(new_dependencies))
            else:
                raise TypeError('Dependencies for {} received neither a string or list: {}'.format(self.identifier, new_dependencies))
        return dependencies


class Bcl2FastQRunner(ModularRunner):
    """
    Produces fastqs for the samples in your samplesheet.  Built-in parameter turns off lane splitting, as the framework makes the assumption
    one sample = one set of fastqs.
    Input: none explicitly!
    Output: fastq
    """
    def get_samples(self):
        sample_id_map = {}
        samples = []
        for line in self.sample_sheet.get("Data"):
            if line.get("Sample_ID") == '' and line.get("Sample_Name") == '':
                continue
            sample  = line.get("Sample_ID") 
            if sample not in sample_id_map:
                #samples.append(SampleTuple(sample, self.sample_sheet.sample_id_to_sample_name(sample)))
                samples.append(SampleTuple(sample, self.sample_sheet.unique_sample_name_map[sample]))
                sample_id_map[sample] = True
        return samples


    def get_output(self, sample):
        """
        Limitations: bcl2fastq2 has insanely varied output given the input.  Currently, we assume that sample_index is consecutively numeric.
        #TODO: detect paired end automatically
        """
        sample_first_instance = {}
        samples_to_return = []
        for line in self.sample_sheet.get("Data"):
            if line.get("Sample_ID") == '' and line.get("Sample_Name") == '':
                continue
            if sample.id ==  line.get("Sample_ID"):
                if not sample in sample_first_instance:
                    sample_first_instance[sample] = line.data_i
                else: #if no_lane_splitting=True this is needed
                    continue
                #if line.has("Lane"): #this is not necessary when no_lane_splitting=True
                #    samples_to_return.append("{path}/{sample_name}_S{sample_index}_L{lane:03d}_R1_001.fastq.gz".format(
                #        path=self.params.self.output_dir, sample_name=line.get("Sample_Name"), sample_index=sample_first_instance[sample], lane=int(line.get("Lane"))))
                #    samples_to_return.append("{path}/{sample_name}_S{sample_index}_L{lane:03d}_R2_001.fastq.gz".format(
                #        path=self.params.self.output_dir, sample_name=line.get("Sample_Name"), sample_index=sample_first_instance[sample], lane=int(line.get("Lane"))))
                #else: #no lane information
                samples_to_return.append("{path}/{sample_name}_S{sample_index}_R1_001.fastq.gz".format(
                    path=self.params.self.output_dir, sample_name=line.get("Sample_Name"), sample_index=sample_first_instance[sample]))
                samples_to_return.append("{path}/{sample_name}_S{sample_index}_R2_001.fastq.gz".format(
                    path=self.params.self.output_dir, sample_name=line.get("Sample_Name"), sample_index=sample_first_instance[sample]))
        print 'z'
        print samples_to_return
        print 'z'
        return {'fastq': samples_to_return}

    def get_dependencies(self, sample):
        return self.bcl2fastq_task

    def workflow(self, workflowRunner):
        if hasattr(self.params.self, 'args'):
            args = " " + self.params.self.optional.args
            if "no-lane-splitting" not in args:
                args+=" --no-lane-splitting"
        else:
            args = '--no-lane-splitting'
        bcl2fastq_wf = Bcl2Fastq(self.params.bcl2fastq_path, self.params.sample_path, self.params.self.output_dir, self.params.sample_sheet, args=args, max_job_cores=self.get_core_count(16))
        self.bcl2fastq_task = workflowRunner.addWorkflowTask(self.identifier, bcl2fastq_wf)

class RSEMRunner(ModularRunner):
    """
    Currently, the RSEM runner only goes from fastq to quants.
    TODOs:
        -Add support for chaining based on the RSEM bam file
        -Add support for running from bams
    Input: fastqs.  The fastqs are assumed to contain r1/r2 information.
    Output: .gene.results files.
    """

    def get_output(self, sample):
        return {'rsem': os.path.join(self.params.self.output_dir,sample.name+".genes.results"), 
                'rsem_model': os.path.join(self.params.self.output_dir,sample.name+".stat",sample.name+".model"),
                'transcript_bam': os.path.join(self.params.self.output_dir,sample.name+".transcript.bam")}

    def define_optionals(self):
        return {'args': ''}

    def workflow(self, workflowRunner):        
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(16)
        for sample in self.collect_samples():
            sample_name = sample.name
            fastq_files = self.collect_input(sample, 'fastq', as_list=True)
            fastq_files = organize_fastqs(fastq_files, self.sample_sheet.is_paired_end())
            if self.sample_sheet.is_paired_end():
                workflowRunner.flowLog(fastq_files)
                rsem_command = "{rsem_path} --paired-end {r1} {r2} {reference} {sample_name} --star -p {cores} --star-path {star_path} --star-gzipped-read-file --temporary-folder {temp_folder}".format( #I have no idea why we need to specify a directory below reference.  So weird.
                    rsem_path=self.params.rsem_path,
                    r1=",".join(fastq_files[0]),
                    r2=",".join(fastq_files[1]),
                    reference=self.params.rsem_annotation,
                    sample_name=os.path.join(self.params.self.output_dir,sample_name),
                    star_path=self.params.star_path[:-5], #-5 due to the star_path variable having the /STAR suffix, which rsem doesn't want
                    genome_dir=self.params.genome,
                    cores=cores,
                    temp_folder=os.path.join(self.params.scratch_path,sample.id))
            else:
                rsem_command = "{rsem_path} {r1} {reference} {sample_name} --star -p {cores} --star-path {star_path} --star-gzipped-read-file --temporary-folder {temp_folder}".format( #I have no idea why we need to specify a directory below reference.  So weird.
                    rsem_path=self.params.rsem_path,
                    r1=",".join(fastq_files),
                    reference=self.params.rsem_annotation,
                    sample_name=os.path.join(self.params.self.output_dir,sample_name),
                    star_path=self.params.star_path[:-5], #-5 due to the star_path variable having the /STAR suffix, which rsem doesn't want
                    genome_dir=self.params.genome,
                    cores=cores,
                    temp_folder=os.path.join(self.params.scratch_path,sample.id))
            rsem_command += ' ' + self.params.self.optional.args
            workflowRunner.flowLog(rsem_command)
            workflowRunner.flowLog("output: {}".format(self.get_output(sample)))
            dependencies = self.collect_dependencies(sample)
            self.task[sample].append(workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id), rsem_command, dependencies=dependencies, nCores=cores, memMb=self.get_memory_count(1024*100)))

class BWARunner(ModularRunner):
    '''
    Runs BWA.
    Input: fastq
    Output: bam
    '''
    def get_output(self, sample):
        return {'bam': os.path.join(self.params.self.output_dir, sample.name, sample.name+".raw.bam")}

    def define_optionals(self):
        return {'args': ''}

    def workflow(self, workflowRunner):
            self.task = defaultdict(list)
            if not os.path.exists(self.params.self.output_dir):
                os.makedirs(self.params.self.output_dir)
            cores = self.get_core_count(20) 
            mem = self.get_memory_count(1024 * 127)
            args = self.params.self.optional.args
            for sample in self.collect_samples():
                sample_name = sample.name
                dependencies = self.collect_dependencies(sample)                
                fastq_files = self.collect_input(sample, 'fastq', as_list=True)
                fastq_files = organize_fastqs(fastq_files, self.sample_sheet.is_paired_end())
                if self.sample_sheet.is_paired_end():
                    if len(fastq_files[0]) != 1 or len(fastq_files[1]) != 1:
                        raise NotImplementedError("bwa only supports one fastq per sample")
                    #put R1, then R2 in a list
                    fastq_files = [x for x in itertools.chain(*fastq_files)]
                    bwa_wf = BWAWorkflow(os.path.join(self.params.self.output_dir, sample_name),
                    self.params.bwa_path, self.params.samtools_path, self.params.genome+"/genome.fa", cores, mem, fastq_files, sample=sample.id, args=args)
                else:
                    if len(fastq_files) != 1:
                        raise NotImplementedError("bwa only supports one fastq per sample: {}".format(fastq_files))
                    bwa_wf = BWAWorkflow(os.path.join(self.params.self.output_dir, sample_name),
                    self.params.bwa_path, self.params.samtools_path, self.params.genome+"/genome.fa", cores, mem, fastq_files, sample=sample.id, args=args)
                bwa_task = workflowRunner.addWorkflowTask('bwa_{}_{}'.format(self.identifier, sample.id), bwa_wf, dependencies=dependencies)
                mv_task = workflowRunner.addTask('mv_{}_{}'.format(self.identifier, sample.id), "mv {} {}".format(os.path.join(self.params.self.output_dir, sample_name, "out.sorted.bam"), os.path.join(self.params.self.output_dir, sample_name, sample_name+".raw.bam")), dependencies=bwa_task, isForceLocal=True)
                self.task[sample].append(workflowRunner.addTask('index_{}_{}'.format(self.identifier, sample.id), "{} index {}".format(self.params.samtools_path, os.path.join(self.params.self.output_dir,sample_name, sample_name+".raw.bam")), dependencies=mv_task))

class CommandLineRunner(ModularRunner):
    '''
    The CommandLineRunner allows the execution of code which is not modularized for zippy.  It uses a simple templating system to create
    sample specific commands.  To run command line runner, you need to specify:
    -input_format and output_format: the filetypes you will require and hand off
    -output from the stage path, how to get to the file outputs.  This command can be templated to use the sample id (sample.id) or sample name (sample.name)
    -command The general command you wish to run.  This command can be templated to use the sample id (sample.id) or sample name (sample.name) for
    per-sample command execution.  It can also be templated with 'self.output' to put the output string into the command
    -input_delimiter: in the case where there is more than one input sample
    TODO: handle cases of non-per-sample execution (merge stages), or initial stages.
    TODO: handle input
    '''
    def get_output(self, sample):
        return {self.params.self.output_format : os.path.join(self.params.self.output_dir, self.create_output_string(sample))}

    def create_output_string(self, sample):
        sample_dict = {"sample.id": sample.id,
                        "sample.name": sample.name}
        return sub_check_wilds(sample_dict, self.params.self.output)

    def create_command_string(self, sample, input_files):
        sample_dict = {"sample.id": sample.id,
                        "sample.name": sample.name,
                        "self.output": create_output_string(sample)}
        return sub_check_wilds(sample_dict, self.params.self.command)


    def workflow(self, workflowRunner):
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(4) 
        mem = self.get_memory_count(1024 * 32)           
        for sample in self.collect_samples():
            dependencies = self.collect_dependencies(sample)
            input_files = self.collect_input(sample, self.params.self.input_format)
            custom_command = create_command_string(sample, input_files)
            self.task[sample].append(workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id),
             custom_command, dependencies=dependencies, nCores=cores, memMb=mem))

class SubsampleBAMRunner(ModularRunner):
    '''
    Uses samtools to sample a percentage of reads from an input bam.
    Input: bam
    Output: bam
    '''
    def get_output(self, sample):
        return {'bam': os.path.join(self.params.self.output_dir, sample.name+".sub.bam")}

    def workflow(self, workflowRunner):
        """
        TODO: might the aligner return more than 1 BAM?
        """
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        for sample in self.collect_samples():
            dependencies = self.collect_dependencies(sample)                
            bam_file = self.collect_input(sample, 'bam')
            new_bam_file = os.path.join(self.params.self.output_dir, sample.name+".sub.bam")
            subsample_command = '{4} view -s {3}{0} -b {1} > {2}'.format(
                self.params.self.subsample_fraction, bam_file, new_bam_file, random.randint(0,100000), self.params.samtools_path)
            self.task[sample].append(workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id),
             subsample_command, dependencies=dependencies))

class BAMtoFASTQRunner(ModularRunner):
    '''
    Uses samtools to convert a bam to a fastq.gz
    TODO: currently paired-end only
    Input: bam
    Output: fastq
    '''
    def get_output(self, sample):
        return {'fastq': [os.path.join(self.params.self.output_dir, sample.name+"_R1_.fastq.gz"), os.path.join(self.params.self.output_dir, sample.name+"_R2_.fastq.gz")]}

    def workflow(self, workflowRunner):
        """
        TODO: might the aligner return more than 1 BAM?
        """
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        for sample in self.collect_samples():
            dependencies = self.collect_dependencies(sample)                
            bam_file = self.collect_input(sample, 'bam')
            new_r1 = os.path.join(self.params.self.output_dir, sample.name+"_R1_.fastq")
            new_r2 = os.path.join(self.params.self.output_dir, sample.name+"_R2_.fastq")
            command = '{samtools} fastq -1 {r1} -2 {r2} {input_bam}'.format(
                samtools=self.params.samtools_path, r1=new_r1, r2=new_r2, input_bam=bam_file)
            fastq_task = workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id),
             command, dependencies=dependencies)
            self.task[sample].append(workflowRunner.addTask('gzip1_{}_{}'.format(self.identifier, sample.id),
             'gzip -f {}'.format(new_r1), dependencies=fastq_task))
            self.task[sample].append(workflowRunner.addTask('gzip2_{}_{}'.format(self.identifier, sample.id),
             'gzip -f {}'.format(new_r2), dependencies=fastq_task))

class PicardAlignStatsRunner(ModularRunner):
    """
    Produces a bunch of picard stats from bwa output.  Currently includes:
    -CollectAlignmentSummaryMetrics
    -CollectInsertSizeMetrics
    -CollectHsMetrics

    """
    def get_output(self, sample):
        return {'align_stats': os.path.join(self.params.self.output_dir,"{}.stats.txt".format(sample.name)), 
            'insert_stats': os.path.join(self.params.self.output_dir,"{}.insert.txt".format(sample.name)),
            'insert_plot': os.path.join(self.params.self.output_dir,"{}.insert.plot.pdf".format(sample.name))}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)        
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        mem = self.get_memory_count(24 * 1024)
        for sample in self.collect_samples():
            sample_scratch = os.path.join(self.params.scratch_path, sample.name+self.identifier)
            if not os.path.exists(sample_scratch):
                os.makedirs(sample_scratch)
            sample_name = sample.name
            bam_file = self.collect_input(sample, 'bam')
            dependencies = self.collect_dependencies(sample)                            
            picard_command = "java -Xmx4G -Djava.io.tmpdir={} -jar {} CollectAlignmentSummaryMetrics R={}/genome.fa I={} O={} TMP_DIR={}".format(
                sample_scratch, self.params.picard, self.params.genome, bam_file, os.path.join(self.params.self.output_dir,"{}.stats.txt".format(sample_name)), sample_scratch)
            stats_task = workflowRunner.addTask('alignstats_{}_{}'.format(self.identifier, sample.id), picard_command, memMb=mem, dependencies=dependencies)        
            self.task[sample].append(stats_task)
            picard_command = "java -Xmx4G -Djava.io.tmpdir={} -jar {} CollectInsertSizeMetrics  I={} O={} H={} M=0.1 TMP_DIR={}".format(
                sample_scratch, self.params.picard, bam_file, os.path.join(self.params.self.output_dir,"{}.insert.txt".format(sample_name)), os.path.join(self.params.self.output_dir,"{}.insert.plot.pdf".format(sample_name)), sample_scratch)
            stats_task = workflowRunner.addTask('insertstats_{}_{}'.format(self.identifier, sample.id), picard_command, memMb=mem, dependencies=dependencies)        
            self.task[sample].append(stats_task)
            if hasattr(self.params.self, 'target_intervals') and hasattr(self.params.self, 'bait_intervals'):
                picard_command = "java -Xmx4G -Djava.io.tmpdir={} -jar {} CollectHsMetrics R={}/genome.fa I={} O={} TMP_DIR={} BAIT_INTERVALS={} TARGET_INTERVALS={}".format(
                    sample_scratch, self.params.picard, self.params.genome, bam_file, os.path.join(self.params.self.output_dir,"{}.stats.txt".format(sample_name)), sample_scratch, self.params.self.optional.bait_intervals, self.params.self.optional.target_intervals)
                stats_task = workflowRunner.addTask('hsstats_{}_{}'.format(self.identifier, sample.id), picard_command, memMb=mem, dependencies=dependencies)        
                self.task[sample].append(stats_task)

class MarkDuplicatesRunner(ModularRunner):
    """
    Runs picard markduplicates.  Currently fixed java heap size to 8GB, as it seems to fail with smaller heap sizes
    Input/output are both bams.
    """
    @passthrough('starlog')
    def get_output(self, sample):
        '''
        Passes through starlog if available.
        '''
        return {'bam': os.path.join(self.params.self.output_dir,"{}.dedup.bam".format(sample.name))}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        mem = self.get_memory_count(50 * 1024)
        for sample in self.collect_samples():
            sample_name = sample.name
            bam_file = self.collect_input(sample, 'bam')
            dependencies = self.collect_dependencies(sample)   
            sample_scratch = os.path.join(self.params.scratch_path, sample.name+self.identifier)
            if self.params.self.optional.use_mate_cigar:
                function = 'MarkDuplicatesWithMateCigar'
            else:
                function = 'MarkDuplicates'
            picard_command = "java -Xmx8G -Djava.io.tmpdir={} -jar {} {} I={} O={} M={} TMP_DIR={}".format(
                sample_scratch, self.params.picard, function, bam_file, os.path.join(self.params.self.output_dir,"{}.dedup.bam".format(sample_name)),
                os.path.join(self.params.self.output_dir,"{}.dedup.stats.txt".format(sample_name)), sample_scratch)
            dedup_task = workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id), picard_command, memMb=mem, dependencies=dependencies)        
            self.task[sample].append(workflowRunner.addTask('index_{}_{}'.format(self.identifier, sample.id), "{} index {}".format(self.params.samtools_path, os.path.join(self.params.self.output_dir, "{}.dedup.bam".format(sample_name))), dependencies=dedup_task))


class MACSRunner(ModularRunner):
    '''
    Calls macs for peak detection.  Defaults to --format BAMPE unless arguments are specified.
    Input: bams
    Output: none consumable by ZIPPY
    '''
    def get_output(self, sample):
        return None

    def define_optionals(self):
        return {'args': '-g hs --format BAMPE'}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)        
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(6)
        mem = self.get_memory_count(32 * 1024)
        for sample in self.collect_samples():
            sample_name = sample.name                    
            bam_file = self.collect_input(sample, 'bam')
            dependencies = self.collect_dependencies(sample)                                        
            macs_command = "{} callpeak --name {} --treatment {} --outdir {}".format(
                self.params.macs_path, sample_name, bam_file, self.params.self.output_dir)
            macs_command += ' ' + self.params.self.optional.args
            workflowRunner.flowLog(macs_command)
            task = workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id), macs_command, nCores=cores, memMb=mem, dependencies=dependencies)
            self.task[sample].append(task)

class DataRunner(ModularRunner):
    '''
    Datarunner loads all the files in its given directory, and provides it as output.  It assumes that sample names will be in the name of relevant files.
    It performs no workflow.

    To get its samples, you must either define self.params.self.samples, a list of samples to load, or self.params.self.sample_sheet, a csv sample sheet that has the
    information about what samples to load.
    For file types that need to be mapped to ZIPPY types, you can use self.params.self.optional.type_map, which is a map from raw file types to types that zippy expects.
    For example, for rsem files going to edger, rsem produces '.genes.results' files, but the proper ZIPPY type is 'rsem'.
    TODO: it should have an option to look recursively through a directory
    '''

    def get_samples(self):
        if hasattr(self.params.self, 'sample_sheet'):
            sample_sheet = SampleSheet(self.params.self.optional.sample_sheet)
            sample_id_map = {}
            samples = []
            for line in sample_sheet.get("Data"):
                if line.get("Sample_ID") == '' and line.get("Sample_Name") == '':
                    continue
                sample  = line.get("Sample_ID") 
                if sample not in sample_id_map:
                    samples.append(SampleTuple(sample, sample_sheet.sample_id_to_sample_name(sample)))
                    sample_id_map[sample] = True
            return samples
        else:
            for sample_name in self.params.self.samples:
                check_valid_samplename(sample_name)
            return [SampleTuple(i,x) for (i,x) in enumerate(self.params.self.samples)]

    def type_map_match(self, fname):
        type_map = self.params.self.optional.type_map
        for (raw_type, zippy_type) in type_map.iteritems():
            if fname.endswith(raw_type):
                return zippy_type
        return False

    def get_output(self, sample):
        output_map = {}
        #print os.path.join(self.params.self.output_dir, '*')
        #print glob.glob(os.path.join(self.params.self.output_dir, '*'))
        for fname in glob.glob(os.path.join(self.params.self.output_dir, '*')):
            sample_name = sample.name
            if sample_name in fname:
                file_split = fname.split('.')
                if hasattr(self.params.self, 'type_map') and self.type_map_match(fname) != False:
                    file_type = self.type_map_match(fname)
                elif file_split[-1] == 'gz':
                    file_type = file_split[-2]
                else:
                    file_type = file_split[-1]
                if file_type in output_map:
                    if not isinstance(output_map[file_type], list):
                        output_map[file_type] = [output_map[file_type]]
                    output_map[file_type].append(fname)
                else:
                    output_map[file_type] = fname
        return output_map

    def get_dependencies(self, sample):
        return []

    def workflow(self, workflowRunner):
        pass

class IndelRealignmentRunner(ModularRunner):
    '''
    Needs to be tested!
    '''
    def get_output(self, sample):
        base_file_name = os.path.basename(self.collect_input(sample.name, 'bam'))
        return {'bam': os.path.join(self.params.self.output_dir,base_file_name)}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)        
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(6)
        mem = self.get_memory_count(12 * 1024)
        for sample in self.collect_samples():
            bam_file = self.collect_input(sample, 'bam')
            base_file_name = os.path.basename(self.collect_input(sample, 'bam'))
            dependencies = self.collect_dependencies(sample)   
            realign_command = "source {mono_source}; {mono} {RI} -bamFiles {bam} -genomeFolders {genome} -outFolder \
            {outFolder}".format(mono_source = self.params.mono_source, mono = self.params.mono, RI = self.params.indelRealign, \
                                bam = bam_file, genome = self.params.genome, outFolder=self.params.self.output_dir)
            ri_task = workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id), realign_command, nCores=cores, memMb=mem, dependencies=dependencies)        
            self.task[sample].append(workflowRunner.addTask('index_{}_{}'.format(self.identifier, sample.id), "{} index {}".format(self.params.samtools_path, os.path.join(self.params.self.output_dir, base_file_name)), dependencies=ri_task))


class PiscesRunner(ModularRunner):
    '''
    Runs the pisces variant caller.
    Note: Pisces takes as input a directory, so all the samples are processed at once.
    TODO: gvcf False support, better cores support, per-sample mode.
    Input: bam
    Output: vcf
    '''
    def get_output(self, sample):
        base_file_name = os.path.basename(self.collect_input(sample, 'bam'))
        out_file_name = '.'.join(base_file_name.split('.')[:-1])+'.genome.vcf'
        return {'vcf': os.path.join(self.params.self.output_dir,out_file_name)}

    def get_dependencies(self, sample):
        return self.task

    def define_optionals(self):
        return {'args': ''}

    def workflow(self, workflowRunner):
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        mem = self.get_memory_count(32 * 1024)
        cores = self.get_core_count(8)        
        args = self.params.self.optional.args
        dependencies = []
        for sample in self.collect_samples():
            bam_file = self.collect_input(sample, 'bam')
            dependencies.extend(self.collect_dependencies(sample))
        bam_dir = os.path.normpath(os.path.join(bam_file, '..'))
        command = "{dotnet} {pisces} -BAMFolder {input_path} -G {genome} -OutFolder {output_path} {args}".format(
            dotnet=self.params.dotnet, pisces=self.params.pisces_path, input_path=bam_dir, genome=self.params.genome, output_path=self.params.self.output_dir, args=args)
        workflowRunner.flowLog(command)
        self.task = workflowRunner.addTask('pisces_{}_{}'.format(self.identifier, sample.id), command, dependencies=dependencies, nCores=cores, memMb=mem)


class StarRunner(ModularRunner):
    '''
    Runs the STAR aligner.  Uses a minimally useful set of default parameters.  Further parameters can be 
    passed via the command line using the 'args' param.
    Input: fastq
    Output: a sorted, indexed bam.
    '''
    def get_output(self, sample):
        return {'bam': os.path.join(self.params.self.output_dir,'{}.raw.bam'.format(sample.name)),
        'starlog': os.path.join(self.params.self.output_dir,'{}Log.final.out'.format(sample.name))}

    def define_optionals(self):
        return {'args': ''}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)        
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(16)
        mem = self.get_memory_count(12 * 1024)        
        args = self.params.self.optional.args
        for sample in self.collect_samples():
            fastq = self.collect_input(sample, 'fastq')
            dependencies = self.collect_dependencies(sample)   
            star_wf = SingleStarFlow(self.params.star_path, self.params.star_index, sample.name, fastq, self.params.self.output_dir, 
                                 max_job_cores=cores, tmp_path=os.path.join(self.params.scratch_path, 'star{}'.format(sample.name)),  command_args=args)
            self.task[sample].append(workflowRunner.addWorkflowTask('star_{}_{}'.format(self.identifier, sample.id), star_wf, dependencies=dependencies))


class FastQCRunner(ModularRunner):
    '''
    Runs fastqc.
    Input: fastq
    Output: none consumable by ZIPPY
    '''
    def get_output(self, sample):
        pass

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)        
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        for sample in self.collect_samples():
            fastq_files = self.collect_input(sample, 'fastq')
            dependencies = self.collect_dependencies(sample)   
            command = "{} {} -o {}".format(self.params.fastqc_path, ' '.join(fastq_files), self.params.self.output_dir)
            self.task[sample].append(workflowRunner.addTask('fastqc_{}_{}'.format(self.identifier, sample.id), command, dependencies=dependencies, memMb=self.get_memory_count(8*1024)))

class MergeBamRunner(ModularRunner):
    '''
    Uses samtools merge to combine a set of bams.  This is our first merge stage, and hence is currently considered experimental.
    Currently, it takes as input a list of sample names, so it does not explicitly depend on the samplesheet.
    TODO: make a merge be many-to-many, instead of many-to-one.  This would involve taking in a list of lists as input.
    TODO: currently we merge to 1 by only returning get_output for our first sample name.  The right way to do this is to modify
    the sample information downstream stages see.  So we must decouple our pipeline from the samplesheet.  Which is probably a good idea anyway.  No offense, Isas.
    '''
    def get_samples(self):
        return [SampleTuple('1', self.identifier)]

    def get_output(self, sample):
        return {'bam': os.path.join(self.params.self.output_dir,'{}.merged.bam'.format(self.identifier))}

    def get_dependencies(self, sample):
        return self.task
        
    def workflow(self, workflowRunner):
        bams = []
        dependencies = []
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        for sample in self.collect_samples():
            if sample.name not in self.params.self.samples:
                continue
            bams.append(self.collect_input(sample, 'bam'))
            dependencies.extend(self.collect_dependencies(sample))
        output_file_path = os.path.join(self.params.self.output_dir,'{}.merged.bam'.format(self.identifier))
        merge_command = '{} merge -f {} {}'.format(self.params.samtools_path, output_file_path, " ".join(bams))
        merge_task = workflowRunner.addTask('mergebam_{}'.format(self.identifier), merge_command, dependencies=dependencies)
        index_command = '{} index {}'.format(self.params.samtools_path, output_file_path)        
        self.task = [workflowRunner.addTask('indexbam_{}'.format(self.identifier), index_command, dependencies=merge_task)]

class RNAQCRunner(ModularRunner):
    '''
    @TODO: make literally everything optional
    Produces RNA-seq stats for your run.  Tested with star.
    Input: bam
    '''
    def get_output(self, sample):
        pass

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)        
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        for sample in self.collect_samples():
            workflowRunner.flowLog('sample {}'.format(sample))
            dependencies = self.collect_dependencies(sample)
            bam_path = self.collect_input(sample, 'bam')
            rsem_model_path = self.collect_input(sample, 'rsem_model')
            sample_scratch = os.path.join(self.params.scratch_path, sample.name+self.identifier)
            if not os.path.exists(sample_scratch):
                os.makedirs(sample_scratch)
            starlog_path = self.collect_input(sample, 'starlog')
            if starlog_path is None: #we did not use star, so we can't get the star stats
                command = "{python} rna_stats.py {bam_path} {stat_path}/{sample_name}.summary.txt --ribosome_bed {ribosome_bed} --intron_bed {intron_bed} --temp_path {temp} ".format(
                python=self.params.python, bam_path=bam_path, stat_path=self.params.self.output_dir, sample_name=sample.name, ribosome_bed=self.params.ribosome_bed, intron_bed=self.params.intron_bed, temp=sample_scratch+'a.out')
            else:
                command = "{python} rna_stats.py {bam_path} {stat_path}/{sample_name}.summary.txt --ribosome_bed {ribosome_bed} --intron_bed {intron_bed} --starlog_path {starlog_path} --temp_path {temp}".format(
                python=self.params.python, bam_path=bam_path, stat_path=self.params.self.output_dir, sample_name=sample.name, ribosome_bed=self.params.ribosome_bed, intron_bed=self.params.intron_bed, starlog_path=starlog_path, temp=sample_scratch+'a.out')
            if hasattr(self.params, 'manifest_bed'):
                command += " --manifest_bed {}".format(self.params.optional.manifest_bed)
            if hasattr(self.params.self, 'dup_stats') and self.params.self.optional.dup_stats:
                command += " --dup_stats"
            if rsem_model_path is not None:
                command += " --rsem_model {}".format(rsem_model_path)
            self.task[sample].append(workflowRunner.addTask('stats_combine_{}'.format(sample.id), command, memMb=40*1024, dependencies=dependencies))
            workflowRunner.flowLog(command)


class SalmonRunner(ModularRunner):
    """
    Input: fastqs.  The fastqs are assumed to contain r1/r2 information.
    Output: .gene.results files.
    """
    def get_output(self, sample):
        #TODO
        return {'rsem': os.path.join(self.params.self.output_dir,sample.name+".genes.results")}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(16)
        mem = self.get_memory_count(100 * 1024)
        for sample in self.collect_samples():
            sample_name = sample.name
            fastq_files = self.collect_input(sample, 'fastq')
            if self.sample_sheet.is_paired_end():
                r1_files = [x for x in fastq_files if '_R1_' in x]
                r2_files = [x for x in fastq_files if '_R2_' in x]
                salmon_command = "{salmon_path} quant -i {salmon_index} -l A -1 {r1} -2 {r2} -o {output_path} --numThreads {cores}".format(
                    salmon_path=self.params.salmon_path,
                    r1=" ".join(r1_files),
                    r2=" ".join(r2_files),
                    reference=self.params.salmon_index,
                    output_path=self.params.self.output_dir)
            else:
                salmon_command = "{salmon_path} quant -i {salmon_index} -l A -1 {r1} -o {output_path} --numThreads {cores}".format(
                    salmon_path=self.params.salmon_path,
                    r1=" ".join(fastq_files),
                    reference=self.params.salmon_index,
                    output_path=self.params.self.output_dir)
            if hasattr(self.params.self, 'args'):
                salmon_command += ' ' + self.params.self.optional.args
            workflowRunner.flowLog(salmon_command)
            dependencies = self.collect_dependencies(sample)
            self.task[sample].append(workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id), salmon_command, dependencies=dependencies, nCores=cores, memMb=mem))

class StrelkaRunner(ModularRunner):
    """
    Strelka call variants from either tumor/normal or germline samples.  To select the mode of strelka, you must specify is_somatic as true or false.
    Input: bam
    Output: vcf
    """
    def get_output(self, sample):
        if self.params.self.is_somatic:
            return {'vcf': os.path.join(self.params.self.output_dir, sample.name+".somatic.vcf.gz")}
        else:
            return {'vcf': os.path.join(self.params.self.output_dir, sample.name+".germline.vcf.gz")}

    def define_optionals(self):
        return {'args': ''}
        
    def workflow(self, workflowRunner):
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(16)
        mem = self.get_memory_count(1024 * 32)
        args = self.params.self.optional.args
        for sample in self.collect_samples():
            #we do this because strelka gets very upset if your directory already exists.  So if you stop your pipeline between strelka configure
            #and strelka run, you would get stuck with an error.
            random_id = ''.join(numpy.random.choice(list(string.digits+string.ascii_lowercase), 6))            
            scratch_path = os.path.join(self.params.scratch_path, sample.name+random_id)
            if os.path.exists(os.path.join(scratch_path, 'runWorkflow.py')):
                os.remove(os.path.join(scratch_path, 'runWorkflow.py'))
            if not os.path.exists(scratch_path):
                os.makedirs(scratch_path)
            dependencies = self.collect_dependencies(sample)                
            bam_file = self.collect_input(sample, 'bam')
            workflowRunner.flowLog('Strelka bams for sample {}: {}'.format(sample.name, bam_file))
            if self.params.self.is_somatic:
                strelka_config = '{} {}/bin/configureStrelkaSomaticWorkflow.py'.format(self.params.python, self.params.strelka_path)
                assert len(bam_file) == 2
                if hasattr(self.params.self, 'normal_pattern'):
                    normal_matches = [x for x in bam_file if self.params.self.optional.normal_pattern in x]
                    tumor_matches = [x for x in bam_file if not self.params.self.optional.normal_pattern in x]
                elif hasattr(self.params.self, 'tumor_pattern'):
                    normal_matches = [x for x in bam_file if not self.params.self.optional.tumor_pattern in x]
                    tumor_matches = [x for x in bam_file if self.params.self.optional.tumor_pattern in x]
                else:
                    raise AttributeError('For somatic strelka, either "normal_pattern" or "tumor_pattern" must be defined.  This argument contains a string that identifies the normal/tumor sample respectively.')
                if len(normal_matches)!=1 or len(tumor_matches)!=1:
                    raise AttributeError('Pattern {} was unable to differentiate bam files: {}'.format(self.params.self.optional.normal_pattern, bam_file))
                bam_string = "--normalBam {} --tumorBam {}".format(normal_matches[0], tumor_matches[0])
            else:
                strelka_config = '{} {}/bin/configureStrelkaGermlineWorkflow.py'.format(self.params.python, self.params.strelka_path)
                bam_string = '--bam {}'.format(bam_file)
            configure_command = '{strelka_config} {bam_string} \
            --referenceFasta={genome}/genome.fa --runDir={scratch_path} --callMemMb={mem} {args}'.format(
                strelka_config=strelka_config, bam_string=bam_string, genome=self.params.genome, 
                scratch_path=scratch_path, mem=mem, args=args)
            sub_task = workflowRunner.addTask('configure_{}_{}'.format(self.identifier, sample.id),
                            configure_command, dependencies=dependencies, isForceLocal=True)
            strelka_command = '{python} {scratch_path}/runWorkflow.py -m local -j {core_count}'.format(
                python=self.params.python, scratch_path=scratch_path, core_count=cores)
            strelka_task = workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id),
                            strelka_command, dependencies=sub_task, nCores=cores)
            if self.params.self.is_somatic:
                module_dir = os.path.abspath(os.path.dirname(__file__))
                merge_path = os.path.join(module_dir, 'vcf_merge.py')
                merge_command = "{python} {merge_path} {scratch_path}/somatic.snvs.vcf.gz {scratch_path}/somatic.indels.vcf.gz {output_dir}/{sample_name}.somatic.vcf.gz".format(
                    python=self.params.python, scratch_path=os.path.join(scratch_path, 'results', 'variants'), output_dir=self.params.self.output_dir, sample_name=sample.name,
                    merge_path=merge_path)
                move_task = workflowRunner.addTask('merge_{}_{}'.format(self.identifier, sample.id),
                            merge_command, dependencies=strelka_task, nCores=1, memMb=4*1024)
            else:
                move_command = "cp {scratch_path}/variants.vcf.gz {output_dir}/{sample_name}.germline.vcf.gz".format(
                    scratch_path=os.path.join(scratch_path, 'results', 'variants'), output_dir=self.params.self.output_dir, sample_name=sample.name)
                move_task = workflowRunner.addTask('move_{}_{}'.format(self.identifier, sample.id),
                            move_command, dependencies=strelka_task, nCores=1, memMb=4*1024, isForceLocal=True)
            self.task[sample].append(workflowRunner.addTask('clean_temp_{}_{}'.format(self.identifier, sample.id),
                "rm -r {}".format(scratch_path), dependencies=move_task, isForceLocal=True))


class EdgerRunner(ModularRunner):
    '''
    This is a merge stage that runs edger to perform differential expression analysis.
    Requires sample_groups, a two element list.  Each element of the list is a list of sample names of one group of the analysis.  The sample
    names can also be wildcards matching unix filename wildcard syntax.
    '''
    def get_samples(self):
        return [SampleTuple('1', self.identifier)]

    def get_output(self, sample):
        pass
        #return {'rsem': os.path.join(self.params.self.output_dir,sample.name+".genes.results")}

    def get_dependencies(self, sample):
        return self.task

    def sample_in_group(self, sample_name, sample_group):
        for group_pattern in sample_group:
            if fnmatch.fnmatch(sample_name, group_pattern):
                return True
        return False

    def workflow(self, workflowRunner):
        rsems_by_group = [ [] , [] ] #it's... an owl?
        dependencies = []
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        temp_path = os.path.join(self.params.scratch_path, self.identifier)
        if not os.path.exists(temp_path):
            os.makedirs(temp_path)
        for sample in self.collect_samples():
            for (i,sample_group) in enumerate(self.params.self.sample_groups):
                if not self.sample_in_group(sample.name, sample_group):
                    continue
                rsems_by_group[i].append(self.collect_input(sample, 'rsem'))
                dependencies.extend(self.collect_dependencies(sample))
        workflowRunner.flowLog('Edger groups: {}'.format(rsems_by_group))
        workflowRunner.flowLog('dependencies: {}'.format(dependencies))
        script_path = os.path.join(zippy_dir, 'run_edger.py')
        output_file_path = os.path.join(self.params.self.output_dir,'{}.edger.out'.format(self.identifier))
        command = '{python} {script_path} --group1 {group1} --group2 {group2} --out_file {out_file} --temp_folder {temp_folder} --r_path {r_path}'.format(
            group1=' '.join(rsems_by_group[0]),
            group2=' '.join(rsems_by_group[1]),
            out_file=output_file_path,
            temp_folder=temp_path,
            r_path=self.params.r_path,
            python=self.params.python,
            script_path=script_path)
        self.task = [workflowRunner.addTask('edger_{}'.format(self.identifier), command, dependencies=dependencies)]

class DeleteRunner(ModularRunner):
    def get_output(self, sample):
        pass

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)        
        for sample in self.collect_samples():
            files_to_delete = []
            dependencies = self.collect_dependencies(sample)
            for file_type in self.params.self.file_types:
                output_for_format = self.collect_input(sample, file_type)
                if isinstance(output_for_format, list):
                        files_to_delete.extend(output_for_format)                    
                else:
                        files_to_delete.append(output_for_format)
            command = 'rm -f {}'.format(' '.join(files_to_delete))
            workflowRunner.flowLog(command)
            self.task[sample].append(workflowRunner.addTask('delete_{}_{}'.format(self.identifier, sample.id), command, dependencies=dependencies))

class BloomSubsampleBAMRunner(ModularRunner):
    '''
    Uses a bloom filter to sample a NUMBER of reads from an input bam.  Bloom filter requires pybloomfilter-mmap package.
    Input: bam
    Output: bam
    '''
    def get_output(self, sample):
        return {'bam': os.path.join(self.params.self.output_dir, sample.name+".sub.bam")}

    def workflow(self, workflowRunner):
        """
        TODO: might the aligner return more than 1 BAM?
        """
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(16)
        for sample in self.collect_samples():
            dependencies = self.collect_dependencies(sample)                
            bam_file = self.collect_input(sample, 'bam')
            new_bam_file = os.path.join(self.params.self.output_dir, sample.name+".sub.bam")
            script_path = os.path.join(zippy_dir, 'downsampling_bloom.py')
            subsample_command = '{python} {script_path} --bam {bam} --downsampled_pairs {count} --output {output_path} --threads {threads}'.format(
                python=self.params.python, script_path=script_path, bam=bam_file, count=self.params.self.reads, output_path=new_bam_file, threads=cores)
            mem = self.get_memory_count(1024 * 32)
            sub_task = workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id),
                            subsample_command, dependencies=dependencies, nCores=cores, memMb=mem)
            self.task[sample].append(workflowRunner.addTask('index_{}_{}'.format(self.identifier, sample.id),
                            "{} index {}".format(self.params.samtools_path, os.path.join(self.params.self.output_dir, sample.name+".sub.bam")), dependencies=sub_task))

class Minimap2Runner(ModularRunner):
    '''
    Runs miniMap2.  Experimental!
    Limitations: single char arg chaining (i.e., -ax instead of -a -x) is not supported.
    args:
    genome_filename (optional): override this to change the name of the genome (default: genome.fa)
    Input: fastq
    Output: if '-a' is in args, it produces sorted, indexed bams.  Otherwise it produces pafs
    '''
    def get_output(self, sample):
        if self.make_bams:
            return {'bam': os.path.join(self.params.self.output_dir, sample.name+".raw.bam")}
        else:
            return {'paf': os.path.join(self.params.self.output_dir, sample.name+".paf")}

    def define_optionals(self):
        return {'genome_filename': 'genome.fa', 'args': ''}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        cores = self.get_core_count(20) 
        mem = self.get_memory_count(1024 * 127)
        genome_suffix = self.params.self.optional.genome_filename
        args = self.params.self.optional.args
        if '-a' in args:
            self.make_bams = True
        for sample in self.collect_samples():
            sample_name = sample.name
            dependencies = self.collect_dependencies(sample)
            fastq_files = self.collect_input(sample, 'fastq', as_list=True)
            fastq_files = organize_fastqs(fastq_files, self.sample_sheet.is_paired_end())
            if self.sample_sheet.is_paired_end():
                #if r1/r2 are separate, we wish to iterleave them:
                #r1_1, r2_1, r1_2, r2_2 etc.  This does that.
                fastq_files = [x for x in itertools.chain(*zip(*fastq_files))]
            if self.make_bams:
                #we pipe to bam and sort
                unsorted_file = os.path.join(self.params.self.output_dir, sample.name+'.unsorted.bam')
                sorted_file = os.path.join(self.params.self.output_dir, sample.name+'.raw.bam')
                output_command = '| {samtools} view -u > '.format(samtools=self.params.samtools_path)
                output_command += unsorted_file
                output_command += ' && {samtools} sort -@ 32 -m 16G {unsorted} > {sorted}'.format(
                        samtools=self.params.samtools_path, unsorted=unsorted_file, sorted=sorted_file)
                output_command += ' && rm {unsorted}'.format(unsorted=unsorted_file)
            else:
                output_command = ' > '+os.path.join(self.params.self.output_dir, sample.name+'.paf')
            command = '{minimap2} {args} {ref} {fastq_files} {output_command}'.format(
                minimap2=self.params.minimap2_path, args=args,
                ref=os.path.join(self.params.genome, genome_suffix),
                fastq_files=" ".join(fastq_files), output_command=output_command)
            minimap_task = workflowRunner.addTask('minimap2_{}_{}'.format(self.identifier, sample.id), command, nCores=cores, memMb=mem, dependencies=dependencies)
            if self.make_bams:
                self.task[sample].append(workflowRunner.addTask('index_{}_{}'.format(self.identifier, sample.id), "{} index {}".format(self.params.samtools_path, os.path.join(self.params.self.output_dir,sample_name+".raw.bam")), dependencies=minimap_task))
            else:
                self.task[sample].append(minimap_task)

class NirvanaRunner(ModularRunner):
    '''
    Uses Nirvana to annotate variants in a vcf file
    '''
    def get_output(self, sample):
        return {'json': os.path.join(self.params.self.output_dir, sample.name + ".json.gz"),
                'json_index': os.path.join(self.params.self.output_dir, sample.name + ".json.gz.jsi"),
                'vcf': os.path.join(self.params.self.output_dir, sample.name + ".vcf.gz")}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)

        args = ''
        if hasattr(self.params.self, 'args'):
            args = self.params.self.optional.args
            
        for sample in self.collect_samples():
            dependencies = self.collect_dependencies(sample)              
            input_vcf = self.collect_input(sample, 'vcf')
            output_path = os.path.join(self.params.self.output_dir, sample.name)
            command = "{dotnet} {nirvana_path} -i {input_vcf} -c {nirvana_cache} -sd {nirvana_supplement} -r {nirvana_ref} -o {output_path} {args}".format(
                dotnet=self.params.dotnet, 
                nirvana_path=self.params.nirvana_path, 
                input_vcf=input_vcf, 
                nirvana_cache=self.params.nirvana_cache, 
                nirvana_supplement=self.params.nirvana_supplement, 
                nirvana_ref=self.params.nirvana_ref, 
                output_path=output_path,
                args=args)
            mem = self.get_memory_count(1024 * 32)
            self.task[sample].append(workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id),
                command, dependencies=dependencies, memMb=mem))

class PrimerDimerMinerRunner(ModularRunner):
    '''
    Runs the included primer dimer miner script.
    Detects reads with more than one of a provided list of primers.
    Input: fastq
    Output: None exposed in ZIPPY.  Produces a file containing pairs of primers that seem to be dimerizing
    '''
    def get_output(self, sample):
        return {}

    def define_optionals(self):
        return {'kmer': 15}

    def workflow(self, workflowRunner):
        self.task = defaultdict(list)
        if not os.path.exists(self.params.self.output_dir):
            os.makedirs(self.params.self.output_dir)
        for sample in self.collect_samples():
            dependencies = self.collect_dependencies(sample)                
            script_path = os.path.join(zippy_dir, 'downsampling_bloom.py')
            fastq_files = self.collect_input(sample, 'fastq', as_list=True)
            pdm_command = '{python} {script_path} --probe_list {probe_list} --kmer {kmer}'.format(
                python=self.params.python, script_path=script_path, probe_list=self.params.self.probe_list,
                kmer=self.params.self.optional.kmer)
            if len(fastq_files) > 2 or len(fastq_files) < 1:
                raise NotImplementedError('PrimerDimerMiner must take 1 or 2 fastqs as input')
            else:
                for (i,x) in enumerate(fastq_files):
                    pdm_command+=' --input_file{} {}'.format(i+1, x)
            self.task[sample].append(workflowRunner.addTask('{}_{}'.format(self.identifier, sample.id),
             pdm_command, dependencies=dependencies))